{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Salary Prediction \n",
    "\n",
    "This notebook serves as the entry point for our salary prediction pipeline.\n",
    "\n",
    "It reads the input dataset, performs cleaning and transformation, trains a model, and evaluates its performance.\n",
    "The core idea is to build a simple but modular pipeline which is flexible enough to incorporate aditional features and models.\n",
    "\n",
    "---\n",
    "\n",
    "##  Table of Contents\n",
    "\n",
    "1. [Imports](#imports)  \n",
    "2. [EDA – Raw Data Check](#eda-raw)  \n",
    "3. [EDA – Visual Exploration](#eda-visual)  \n",
    "4. [Preprocessing](#preprocessing)  \n",
    "5. [Feature Transformation](#features)  \n",
    "6. [Model Training & Evaluation](#model)\n",
    "7. [Predicting on New Input Data](#7-predicting-on-new-input-data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1. Imports\n",
    "\n",
    "All core libraries (Pandas, NumPy, Seaborn...) and our own modular code from `src/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "from src.preprocessing import prepare_data\n",
    "from src import eda\n",
    "from src.features import transform_features\n",
    "from src.model import split_data, train_model, evaluate_model\n",
    "from src.predict import predict_from_csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2. EDA – Raw Data Check\n",
    "\n",
    "Before running the pipeline, we quickly check for:\n",
    "- Consistency between the two CSV files (people & salary)\n",
    "- Null values\n",
    "- General data structure\n",
    "\n",
    "This is optional but helps ensure the data looks clean enough to proceed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)  \n",
    "pd.set_option('display.max_columns', None)  \n",
    "pd.set_option('display.width', None)  \n",
    "pd.set_option('display.expand_frame_repr', False)  \n",
    "\n",
    "#Load raw data (people and salary info)\n",
    "df_people = pd.read_csv(\"data/people.csv\")\n",
    "df_salary = pd.read_csv(\"data/salary.csv\")\n",
    "\n",
    "#Check consistency in both df\n",
    "eda.check_id_consistency(df_people, df_salary)\n",
    "\n",
    "#Check how many nulls are present in people.csv\n",
    "eda.check_nulls(df_people, name=\"people.csv\")\n",
    "\n",
    "\n",
    "df_merged = df_people.merge(df_salary, on=\"id\", how=\"left\")\n",
    "\n",
    "# Check how many rows of salary has nulls.\n",
    "eda.count_salary_nulls(df_merged)\n",
    "\n",
    "# Count and display rows that have at least one null value\n",
    "eda.count_rows_with_any_null(df_merged, name=\"merged df\")\n",
    "\n",
    "#Print shape, types and head of the merged dataset\n",
    "eda.print_df_overview(df_merged, name=\"merged df\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3. EDA – Visual Exploration\n",
    "\n",
    "Here we look at the distribution of variables like Age, Salary, and Years of Experience.\n",
    "\n",
    "Also checks how many job titles appear more than a threshold.  \n",
    "It helps us decide which ones to group under \"Other\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reload data for visualization purposes\n",
    "df_people = pd.read_csv(\"data/people.csv\")\n",
    "df_salary = pd.read_csv(\"data/salary.csv\")\n",
    "\n",
    "df_merged_exp = df_people.merge(df_salary, on=\"id\", how=\"left\")\n",
    "\n",
    "\n",
    "# Drop null rows and apply log transform to salary (based on earlier EDA)\n",
    "df_clean_exp = df_merged_exp.dropna().copy()\n",
    "df_clean_exp[\"Salary_log\"] = np.log(df_clean_exp[\"Salary\"])\n",
    "\n",
    "#Plotting different data distributions.\n",
    "eda.plot_distributions(df_clean_exp)\n",
    "eda.count_job_titles(df_clean_exp, threshold=6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Preprocessing\n",
    "\n",
    "Loads and merges both datasets, removes null rows, and adds a log-transformed Salary column.\n",
    "\n",
    "Everything here is done through `prepare_data()` inside `src/preprocessing.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and clean data (null removal + log transform on Salary)\n",
    "\n",
    "df_clean = prepare_data(\"data/people.csv\", \"data/salary.csv\")\n",
    "#df_clean.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  5. Feature Transformation\n",
    "\n",
    "We apply:\n",
    "- One-hot encoding to `Education Level`\n",
    "- Job Title grouping (threshold-based)\n",
    "\n",
    "Handled by `transform_features()` inside `src/features.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "job_title_threshold = 3  #Minimum count to keep job title (else grouped as \"other\")\n",
    "X, y = transform_features(df_clean, job_threshold=job_title_threshold)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  6. Model Training & Evaluation\n",
    "\n",
    "We train a Linear Regression model and evaluate its performance using:\n",
    "- MAE & RMSE\n",
    "- 95% confidence intervals (via bootstrap)\n",
    "- Comparison with a DummyRegressor\n",
    "\n",
    "All metrics are printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = split_data(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Train linear regression model\n",
    "model = train_model(X_train, y_train)\n",
    "\n",
    "# Evaluate performance with MAE, RMSE and 95% CI ( with bootstrap)\n",
    "evaluate_model(model, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Predicting on New Input Data\n",
    "\n",
    "If a CSV file containing new records is found (named `predict_sample.csv`), we apply the trained model to generate predicted salaries.\n",
    "\n",
    "This allows the user to calculate salaries using different independant variables.\n",
    "\n",
    "If the file is not found, the block is skipped safely.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prediction_file = \"data/predict_sample.csv\"                     \n",
    "\n",
    "# Check if file exists\n",
    "if os.path.exists(prediction_file):\n",
    "    print(\"Prediction file found. Running predictions\\n\")\n",
    "\n",
    "    new_predictions = predict_from_csv(prediction_file, model, job_threshold=job_title_threshold)\n",
    "\n",
    "    # Show results\n",
    "    print(\"Predicted salaries for new input:\\n\")\n",
    "    print(new_predictions[[\"Age\", \"Education Level\", \"Job Title\", \"Years of Experience\", \"Predicted Salary\"]])\n",
    "\n",
    "else:\n",
    "    print(f\"File '{prediction_file}' not found. Skipping prediction block.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
